{
 "cells": [
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T04:04:32.281099Z",
     "start_time": "2025-09-05T04:04:30.384913Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f\"Torch version {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version 2.7.1+cu118\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e213c59c66e14e48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:38:11.026868Z",
     "start_time": "2025-09-05T05:32:53.496078Z"
    }
   },
   "source": [
    "import importlib\n",
    "import models.nlp\n",
    "importlib.reload(models.nlp)\n",
    "from models.nlp import SimpleTextRnn, SimpleTransformerDecoderOnly\n",
    "\n",
    "from config import ROOT_DIR\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets.alice_in_wonderland\n",
    "importlib.reload(datasets.alice_in_wonderland)\n",
    "\n",
    "# options\n",
    "#mode = 'RNN'\n",
    "mode = 'Transformer'\n",
    "\n",
    "# hyperparameters\n",
    "embedding_size = 256\n",
    "hidden_size = 32\n",
    "lr = 0.0025\n",
    "seq_length = 10\n",
    "vocab_size = 500\n",
    "\n",
    "dataset_train = None\n",
    "dataset_test = None\n",
    "\n",
    "dataset_train = datasets.alice_in_wonderland.AliceInWonderlandDataset(seq_length=seq_length, vocab_size=vocab_size, train=True)\n",
    "tokenizer = dataset_train.tokenizer\n",
    "dataset_test = datasets.alice_in_wonderland.AliceInWonderlandDataset(seq_length=seq_length, vocab_size=vocab_size, train=False, tokenizer=tokenizer)\n",
    "\n",
    "log_vocab = math.log(tokenizer.get_vocab_size())\n",
    "print(f\"Log of vocab {log_vocab}\")\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=64, shuffle=True)\n",
    "\n",
    "if mode == 'RNN':\n",
    "    model = SimpleTextRnn(vocab_size=tokenizer.get_vocab_size(), embed_size=embedding_size, hidden_size=hidden_size).to(device)\n",
    "elif mode == 'Transformer':\n",
    "    model = SimpleTransformerDecoderOnly(vocab_size=tokenizer.get_vocab_size(), embedding_dim=embedding_size, num_heads=4, num_layers=2, ff_dimension=128, max_length=seq_length, dropout=0.1).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "train_epoch_count = 100\n",
    "\n",
    "if not os.path.exists(os.path.join(ROOT_DIR, '.models')):\n",
    "    os.mkdir(os.path.join(ROOT_DIR, '.models'))\n",
    "\n",
    "trained_path = os.path.join(ROOT_DIR, '.models', 'alice_in_wonderland_trained.pth')\n",
    "untrained_path = os.path.join(ROOT_DIR, '.models', 'alice_in_wonderland_untrained.pth')\n",
    "\n",
    "torch.save(model.state_dict(), untrained_path)\n",
    "no_improvement_count = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(train_epoch_count+1):\n",
    "        model.train()\n",
    "        for batch_X, batch_Y in train_dataloader:\n",
    "            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "\n",
    "            loss = criterion(outputs, batch_Y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for val_X, val_Y in test_dataloader:\n",
    "                val_X, val_Y = val_X.to(device), val_Y.to(device)\n",
    "                val_output = model(val_X)\n",
    "                val_loss = criterion(val_output, val_Y)\n",
    "                val_losses.append(val_loss.item())\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f\"Epoch {epoch} | Train Loss: {loss.item():.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            no_improvement_count = 0\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), trained_path)\n",
    "            best_epoch = epoch\n",
    "            print(f\"New best model at epoch {epoch} | val_loss: {avg_val_loss:.4f}\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            if no_improvement_count >= 5:\n",
    "                print(f\"Stopping training after {no_improvement_count} epochs without improvement\")\n",
    "                break\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping training due to keyboard interrupt\")\n",
    "\n",
    "print(f\"Using model from epoch {best_epoch} | val_loss: {best_val_loss:.4f} | entropy vocab: {log_vocab:.4f}\")\n",
    "\n",
    "\n",
    "current_datetime = datetime.now()\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "formatted_date = current_datetime.strftime(\"%Y_%m_%d\")\n",
    "with open(os.path.join(ROOT_DIR, '.models', f\"{formatted_date}_alice_in_wonderland_training_log.txt\"), 'a') as f:\n",
    "    f.write(f\"{formatted_datetime}: Train result\\n\")\n",
    "    f.write(f\"Mode: {mode}\\n\")\n",
    "    f.write(f\"Vocab size: {tokenizer.get_vocab_size()}\\n\")\n",
    "    f.write(f\"Entropy of vocab: {log_vocab:.4f}\\n\")\n",
    "    f.write(f\"Embedding size: {embedding_size}\\n\")\n",
    "    f.write(f\"Hidden size: {hidden_size}\\n\")\n",
    "    f.write(f\"Seq length: {dataset_train.seq_length}\\n\")\n",
    "    f.write(f\"Train epochs: {train_epoch_count}\\n\")\n",
    "    f.write(f\"Learning rate: {lr}\\n\")\n",
    "    f.write(f\"Best epoch: {best_epoch}\\n\")\n",
    "    f.write(f\"Best val loss: {best_val_loss:.4f}\\n\")\n",
    "    f.write(\"***\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log of vocab 6.214608098422191\n",
      "Epoch 0 | Train Loss: 5.8170 | Val Loss: 5.6404\n",
      "New best model at epoch 0 | val_loss: 5.6404\n",
      "Epoch 1 | Train Loss: 5.5702 | Val Loss: 5.2805\n",
      "New best model at epoch 1 | val_loss: 5.2805\n",
      "Epoch 2 | Train Loss: 5.4175 | Val Loss: 5.0783\n",
      "New best model at epoch 2 | val_loss: 5.0783\n",
      "Epoch 3 | Train Loss: 5.2295 | Val Loss: 4.9453\n",
      "New best model at epoch 3 | val_loss: 4.9453\n",
      "Epoch 4 | Train Loss: 4.8739 | Val Loss: 4.9173\n",
      "New best model at epoch 4 | val_loss: 4.9173\n",
      "Epoch 5 | Train Loss: 4.4088 | Val Loss: 4.8631\n",
      "New best model at epoch 5 | val_loss: 4.8631\n",
      "Epoch 6 | Train Loss: 4.6895 | Val Loss: 4.8101\n",
      "New best model at epoch 6 | val_loss: 4.8101\n",
      "Epoch 7 | Train Loss: 4.1526 | Val Loss: 4.7598\n",
      "New best model at epoch 7 | val_loss: 4.7598\n",
      "Epoch 8 | Train Loss: 4.8414 | Val Loss: 4.7209\n",
      "New best model at epoch 8 | val_loss: 4.7209\n",
      "Epoch 9 | Train Loss: 4.5583 | Val Loss: 4.7266\n",
      "Epoch 10 | Train Loss: 5.0736 | Val Loss: 4.7213\n",
      "Epoch 11 | Train Loss: 4.4614 | Val Loss: 4.6685\n",
      "New best model at epoch 11 | val_loss: 4.6685\n",
      "Epoch 12 | Train Loss: 4.4287 | Val Loss: 4.6686\n",
      "Epoch 13 | Train Loss: 4.5790 | Val Loss: 4.6320\n",
      "New best model at epoch 13 | val_loss: 4.6320\n",
      "Epoch 14 | Train Loss: 4.6711 | Val Loss: 4.6331\n",
      "Epoch 15 | Train Loss: 4.6902 | Val Loss: 4.6158\n",
      "New best model at epoch 15 | val_loss: 4.6158\n",
      "Epoch 16 | Train Loss: 4.8511 | Val Loss: 4.5806\n",
      "New best model at epoch 16 | val_loss: 4.5806\n",
      "Epoch 17 | Train Loss: 4.8571 | Val Loss: 4.5914\n",
      "Epoch 18 | Train Loss: 3.6915 | Val Loss: 4.5669\n",
      "New best model at epoch 18 | val_loss: 4.5669\n",
      "Epoch 19 | Train Loss: 4.7716 | Val Loss: 4.5379\n",
      "New best model at epoch 19 | val_loss: 4.5379\n",
      "Epoch 20 | Train Loss: 3.9844 | Val Loss: 4.5290\n",
      "New best model at epoch 20 | val_loss: 4.5290\n",
      "Epoch 21 | Train Loss: 4.5012 | Val Loss: 4.5119\n",
      "New best model at epoch 21 | val_loss: 4.5119\n",
      "Epoch 22 | Train Loss: 5.0049 | Val Loss: 4.4852\n",
      "New best model at epoch 22 | val_loss: 4.4852\n",
      "Epoch 23 | Train Loss: 4.0305 | Val Loss: 4.4818\n",
      "New best model at epoch 23 | val_loss: 4.4818\n",
      "Epoch 24 | Train Loss: 4.4811 | Val Loss: 4.4701\n",
      "New best model at epoch 24 | val_loss: 4.4701\n",
      "Epoch 25 | Train Loss: 4.2035 | Val Loss: 4.4478\n",
      "New best model at epoch 25 | val_loss: 4.4478\n",
      "Epoch 26 | Train Loss: 3.9841 | Val Loss: 4.4400\n",
      "New best model at epoch 26 | val_loss: 4.4400\n",
      "Epoch 27 | Train Loss: 4.5112 | Val Loss: 4.4368\n",
      "New best model at epoch 27 | val_loss: 4.4368\n",
      "Epoch 28 | Train Loss: 4.5739 | Val Loss: 4.4233\n",
      "New best model at epoch 28 | val_loss: 4.4233\n",
      "Epoch 29 | Train Loss: 4.2838 | Val Loss: 4.4206\n",
      "New best model at epoch 29 | val_loss: 4.4206\n",
      "Epoch 30 | Train Loss: 4.8528 | Val Loss: 4.4264\n",
      "Epoch 31 | Train Loss: 3.8250 | Val Loss: 4.4125\n",
      "New best model at epoch 31 | val_loss: 4.4125\n",
      "Epoch 32 | Train Loss: 4.7467 | Val Loss: 4.3896\n",
      "New best model at epoch 32 | val_loss: 4.3896\n",
      "Epoch 33 | Train Loss: 4.7282 | Val Loss: 4.3845\n",
      "New best model at epoch 33 | val_loss: 4.3845\n",
      "Epoch 34 | Train Loss: 3.8153 | Val Loss: 4.3769\n",
      "New best model at epoch 34 | val_loss: 4.3769\n",
      "Epoch 35 | Train Loss: 3.9163 | Val Loss: 4.3462\n",
      "New best model at epoch 35 | val_loss: 4.3462\n",
      "Epoch 36 | Train Loss: 4.2576 | Val Loss: 4.3776\n",
      "Epoch 37 | Train Loss: 3.9178 | Val Loss: 4.3382\n",
      "New best model at epoch 37 | val_loss: 4.3382\n",
      "Epoch 38 | Train Loss: 3.9556 | Val Loss: 4.3567\n",
      "Epoch 39 | Train Loss: 3.8745 | Val Loss: 4.3395\n",
      "Epoch 40 | Train Loss: 3.4214 | Val Loss: 4.3266\n",
      "New best model at epoch 40 | val_loss: 4.3266\n",
      "Epoch 41 | Train Loss: 4.0662 | Val Loss: 4.3094\n",
      "New best model at epoch 41 | val_loss: 4.3094\n",
      "Epoch 42 | Train Loss: 4.1952 | Val Loss: 4.3134\n",
      "Epoch 43 | Train Loss: 4.3860 | Val Loss: 4.3141\n",
      "Epoch 44 | Train Loss: 3.7923 | Val Loss: 4.3247\n",
      "Epoch 45 | Train Loss: 3.8493 | Val Loss: 4.2962\n",
      "New best model at epoch 45 | val_loss: 4.2962\n",
      "Epoch 46 | Train Loss: 4.1071 | Val Loss: 4.2880\n",
      "New best model at epoch 46 | val_loss: 4.2880\n",
      "Epoch 47 | Train Loss: 3.8016 | Val Loss: 4.2813\n",
      "New best model at epoch 47 | val_loss: 4.2813\n",
      "Epoch 48 | Train Loss: 4.0825 | Val Loss: 4.3117\n",
      "Epoch 49 | Train Loss: 4.1647 | Val Loss: 4.2958\n",
      "Epoch 50 | Train Loss: 3.5114 | Val Loss: 4.2748\n",
      "New best model at epoch 50 | val_loss: 4.2748\n",
      "Epoch 51 | Train Loss: 3.7350 | Val Loss: 4.2549\n",
      "New best model at epoch 51 | val_loss: 4.2549\n",
      "Epoch 52 | Train Loss: 4.2842 | Val Loss: 4.2600\n",
      "Epoch 53 | Train Loss: 3.5495 | Val Loss: 4.2695\n",
      "Epoch 54 | Train Loss: 4.4618 | Val Loss: 4.2716\n",
      "Epoch 55 | Train Loss: 4.0116 | Val Loss: 4.2560\n",
      "Epoch 56 | Train Loss: 3.4740 | Val Loss: 4.2415\n",
      "New best model at epoch 56 | val_loss: 4.2415\n",
      "Epoch 57 | Train Loss: 3.7298 | Val Loss: 4.2488\n",
      "Epoch 58 | Train Loss: 3.8779 | Val Loss: 4.2279\n",
      "New best model at epoch 58 | val_loss: 4.2279\n",
      "Epoch 59 | Train Loss: 3.9631 | Val Loss: 4.2249\n",
      "New best model at epoch 59 | val_loss: 4.2249\n",
      "Epoch 60 | Train Loss: 3.3846 | Val Loss: 4.2317\n",
      "Epoch 61 | Train Loss: 3.8089 | Val Loss: 4.2328\n",
      "Epoch 62 | Train Loss: 3.6031 | Val Loss: 4.2226\n",
      "New best model at epoch 62 | val_loss: 4.2226\n",
      "Epoch 63 | Train Loss: 3.2644 | Val Loss: 4.2233\n",
      "Epoch 64 | Train Loss: 4.3822 | Val Loss: 4.2317\n",
      "Epoch 65 | Train Loss: 3.5373 | Val Loss: 4.2134\n",
      "New best model at epoch 65 | val_loss: 4.2134\n",
      "Epoch 66 | Train Loss: 3.5110 | Val Loss: 4.2321\n",
      "Epoch 67 | Train Loss: 3.9060 | Val Loss: 4.2398\n",
      "Epoch 68 | Train Loss: 4.0293 | Val Loss: 4.2496\n",
      "Epoch 69 | Train Loss: 3.6399 | Val Loss: 4.2368\n",
      "Epoch 70 | Train Loss: 3.8711 | Val Loss: 4.2322\n",
      "Stopping training after 5 epochs without improvement\n",
      "Using model from epoch 65 | val_loss: 4.2134 | entropy vocab: 6.2146\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "40402809658d22c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T05:38:22.779646Z",
     "start_time": "2025-09-05T05:38:22.251232Z"
    }
   },
   "source": [
    "from tokenizers.decoders import Metaspace as MetaspaceDecoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer.decoder = MetaspaceDecoder(replacement=\" \", prepend_scheme=\"never\")\n",
    "\n",
    "seq_length = dataset_train.seq_length\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def generate_text(input, num_tokens, path):\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenizer.encode(input).ids\n",
    "    leftover_prefix = []\n",
    "    if len(input_ids) > seq_length:\n",
    "        leftover_prefix = input_ids[:-seq_length]\n",
    "        input_ids = input_ids[-seq_length:]\n",
    "\n",
    "    generated = input_ids.copy()\n",
    "    for _ in range(num_tokens):\n",
    "        input_tensor = torch.tensor([generated[-seq_length:]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = F.softmax(output / 0.2, dim=1)\n",
    "            next_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "        generated.append(next_id)\n",
    "    return tokenizer.decode(leftover_prefix + generated).replace(\"▁\", \" \")\n",
    "\n",
    "def print_next_token_probabilities(input, path):\n",
    "    \"\"\"\n",
    "    Print the top 10 next token probabilities\n",
    "    :param input:\n",
    "    :param path:\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenizer.encode(input).ids\n",
    "    if len(input_ids) > seq_length:\n",
    "        input_ids = input_ids[-seq_length:]\n",
    "\n",
    "    generated = input_ids.copy()\n",
    "    input_tensor = torch.tensor([generated[-seq_length:]], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        top_probs, top_indices = torch.topk(probabilities[0], 10)\n",
    "    \n",
    "    for i in range(10):\n",
    "      token_id = top_indices[i].item()\n",
    "      prob = top_probs[i].item()\n",
    "      token_text = tokenizer.decode([token_id])  # Assuming you have a tokenizer\n",
    "      print(f\"Token: '{token_text}' (ID: {token_id}) - Probability: {prob:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Output and various stats\n",
    "prompt = clean_text(\"Oh, you can't help that; we're all \")\n",
    "\n",
    "encoded = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "token_ids = encoded.ids\n",
    "tokens = [tokenizer.decode([t]) for t in token_ids]\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Prompt tokens: {tokens}\\n\")\n",
    "\n",
    "print_next_token_probabilities(prompt, trained_path)\n",
    "\n",
    "result_untrained = generate_text(prompt, 100, untrained_path)\n",
    "result_trained = generate_text(prompt, 100, trained_path)\n",
    "print(f\"Untrained sample: {result_untrained}\\n\")\n",
    "print(f\"Trained sample: {result_trained}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: oh, you can't help that; we're all \n",
      "\n",
      "Prompt tokens: ['▁o', 'h,', '▁you', '▁can', \"'t\", '▁he', 'l', 'p', '▁that', ';', '▁w', 'e', \"'re\", '▁all', '▁']\n",
      "\n",
      "Token: 'ought' (ID: 231) - Probability: 0.1276\n",
      "Token: 'ver' (ID: 119) - Probability: 0.1043\n",
      "Token: 'x' (ID: 47) - Probability: 0.0937\n",
      "Token: ''t' (ID: 142) - Probability: 0.0696\n",
      "Token: '1' (ID: 10) - Probability: 0.0672\n",
      "Token: 'our' (ID: 224) - Probability: 0.0667\n",
      "Token: 'id' (ID: 80) - Probability: 0.0635\n",
      "Token: 'z' (ID: 49) - Probability: 0.0594\n",
      "Token: '4' (ID: 13) - Probability: 0.0528\n",
      "Token: 'ed' (ID: 73) - Probability: 0.0391\n",
      "\n",
      "\n",
      "Untrained sample:  oh, you can't help that; we're all  mock king when0ackveroutverose theirow.'t mock neverout de wheni back litppureenamesverose inxy,lyly, shaver'tverunverose not marchkedust when0 when0 when a got \"andome whenance,\"at't gollss lastceningup \"nroortroortverose sa su lj ne when't she alice, lastag neveroutver kn alice, ducheovenonkedw g ab. shaackver\n",
      "\n",
      "Trained sample:  oh, you can't help that; we're all ought to beated tone of the queen's a tred, and she face, and the queen's a little pleaseing to the queen's a little pleaseing tone of the queen's a little pleaseing to the figer, and she was a surors and the queen's the might left of the other, and she silly the queen's no time she had nothing the tea ta\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T04:04:33.118910700Z",
     "start_time": "2025-09-04T04:58:48.243507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ],
   "id": "66aeb327295dcc35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight Parameter containing:\n",
      "tensor([[-0.9728,  0.4297,  0.1628,  ...,  1.3895, -0.0952, -1.7533],\n",
      "        [-1.6825,  0.7517, -0.2961,  ...,  0.3894,  2.3665, -0.5458],\n",
      "        [ 1.0750, -0.3850, -0.0465,  ...,  1.3011,  0.4996, -0.9055],\n",
      "        ...,\n",
      "        [ 1.2738,  0.2364, -1.5954,  ...,  0.7503, -0.8904,  1.3999],\n",
      "        [ 0.9704, -0.0747,  0.5486,  ..., -0.3408,  0.2730,  0.4335],\n",
      "        [ 0.6290, -0.6324, -0.5008,  ...,  0.4935, -0.6753,  0.5932]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "rnn.weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.2566,  0.0724,  0.0699,  ..., -0.2430, -0.2904,  0.1087],\n",
      "        [-0.3271,  0.0460, -0.2292,  ...,  0.4650, -0.4118,  0.0568],\n",
      "        [ 0.0422,  0.0064,  0.0764,  ...,  0.5999, -0.2273,  0.0566],\n",
      "        ...,\n",
      "        [ 0.1442, -0.7310, -0.4212,  ..., -0.1402,  0.2384, -0.3092],\n",
      "        [ 0.0835, -0.1230,  0.1182,  ...,  0.4482, -0.5176, -0.1190],\n",
      "        [ 0.3978,  0.3557, -0.0986,  ...,  0.1547, -0.0208, -0.2164]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "rnn.weight_hh_l0 Parameter containing:\n",
      "tensor([[ 2.9028e-01, -1.0994e-01,  4.2946e-01,  ...,  4.5798e-01,\n",
      "         -4.6851e-01,  8.6773e-03],\n",
      "        [ 5.6652e-01,  7.0337e-01,  3.0030e-01,  ...,  3.8612e-01,\n",
      "         -2.7137e-01, -7.2731e-02],\n",
      "        [ 1.0804e-04,  2.4919e-02,  1.1771e-01,  ...,  3.1302e-01,\n",
      "         -2.3047e-01, -6.0261e-01],\n",
      "        ...,\n",
      "        [-1.6737e-02,  1.9064e-01, -5.2220e-02,  ..., -1.1240e-01,\n",
      "          2.0549e-01,  5.8883e-01],\n",
      "        [ 2.3035e-01,  3.1929e-01,  2.4383e-01,  ...,  4.5185e-01,\n",
      "          1.1889e-02, -3.5365e-01],\n",
      "        [ 5.5236e-01,  7.2769e-02,  1.0355e-01,  ...,  2.8031e-01,\n",
      "         -5.1307e-01,  1.7036e-01]], device='cuda:0', requires_grad=True)\n",
      "rnn.bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.4236,  0.4292,  0.4972,  0.3949,  0.2204,  0.3698,  0.3036,  0.3990,\n",
      "         0.3591,  0.2828,  0.4940,  0.6129,  0.4559,  0.3162,  0.3946,  0.4603,\n",
      "         0.2902,  0.5684,  0.4247,  0.4565,  0.3193,  0.4954,  0.5286,  0.3637,\n",
      "         0.2441,  0.4736,  0.4789,  0.1621,  0.4106,  0.4986,  0.3620,  0.5287,\n",
      "         0.3859,  0.6357,  0.6385,  0.2644,  0.5199,  0.5391,  0.2517,  0.4301,\n",
      "         0.4088,  0.2668,  0.4016,  0.3507,  0.4674,  0.4068,  0.4466,  0.4857,\n",
      "         0.4076,  0.5309,  0.5640,  0.5833,  0.3973,  0.2526,  0.3630,  0.3419,\n",
      "         0.4618,  0.2226,  0.3223,  0.2128,  0.5316,  0.2075,  0.3461,  0.3786,\n",
      "         0.1654,  0.0161,  0.3474, -0.0480,  0.2308, -0.1316,  0.2365,  0.1494,\n",
      "        -0.0213, -0.2466,  0.0515,  0.0039, -0.2523, -0.0462, -0.0609,  0.4547,\n",
      "        -0.0376, -0.2202,  0.3956,  0.1924, -0.1719, -0.3071, -0.1062,  0.2644,\n",
      "        -0.1092, -0.1001, -0.2675,  0.2460,  0.1025, -0.0168, -0.0412, -0.1992,\n",
      "         0.1881,  0.0178, -0.1848,  0.0403, -0.2442, -0.2067,  0.2129, -0.0467,\n",
      "         0.1613,  0.0120,  0.2493,  0.1626,  0.2524,  0.1161, -0.1649, -0.2129,\n",
      "        -0.0818, -0.2446, -0.1722, -0.0329,  0.2607,  0.3898,  0.1306, -0.0094,\n",
      "        -0.1553,  0.0898, -0.1535, -0.0571, -0.0080,  0.2030, -0.0176,  0.1964,\n",
      "         0.2449,  0.2129,  0.2999, -0.1356,  0.4319,  0.1295,  0.1667, -0.1802,\n",
      "        -0.2368, -0.1280,  0.1484, -0.1348, -0.0994, -0.0814, -0.1326, -0.2537,\n",
      "        -0.1441,  0.0227,  0.4018, -0.2088,  0.1153, -0.1199,  0.0105, -0.3269,\n",
      "         0.0370, -0.0777,  0.2084, -0.3148, -0.1311,  0.2946,  0.1773, -0.0056,\n",
      "         0.1879,  0.0983, -0.0382,  0.2477,  0.0604,  0.0132,  0.1968,  0.0569,\n",
      "        -0.1344,  0.1547, -0.3521,  0.2378,  0.3468, -0.2567, -0.1160,  0.0393,\n",
      "         0.2689, -0.2316,  0.0789,  0.0855,  0.3477, -0.1696, -0.0806, -0.2533,\n",
      "        -0.0624,  0.3515,  0.0622, -0.1592,  0.1965,  0.3020, -0.2130, -0.2043,\n",
      "         0.3890,  0.6278,  0.1867,  0.5776,  0.1337,  0.5960,  0.1525,  0.5999,\n",
      "         0.3075,  0.5284,  0.5708,  0.4163,  0.6276,  0.5579,  0.6343,  0.1782,\n",
      "         0.5738,  0.6474, -0.0104,  0.3374,  0.5305,  0.5360,  0.6429,  0.1243,\n",
      "         0.4400,  0.4292,  0.5121,  0.1735,  0.5236,  0.2450,  0.5411,  0.7712,\n",
      "         0.3010,  0.6266,  0.7630,  0.2840,  0.8075,  0.5874,  0.2745,  0.6023,\n",
      "         0.4656,  0.3402,  0.0740,  0.3929, -0.0272,  0.4843,  0.7324,  0.7233,\n",
      "         0.5255,  0.5184,  0.8504,  0.7876,  0.1523,  0.0692,  0.5701,  0.5878,\n",
      "         0.4566, -0.0718,  0.6174,  0.2658,  0.4573,  0.0553,  0.5780,  0.3113],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "rnn.bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.5425,  0.5881,  0.3647,  0.4477,  0.3708,  0.4618,  0.3016,  0.3178,\n",
      "         0.2557,  0.2759,  0.2904,  0.4659,  0.4074,  0.2308,  0.4712,  0.4616,\n",
      "         0.1761,  0.6648,  0.2918,  0.4214,  0.4322,  0.6802,  0.4323,  0.3883,\n",
      "         0.3663,  0.5971,  0.4653,  0.3252,  0.3736,  0.4578,  0.4993,  0.5377,\n",
      "         0.2343,  0.5015,  0.5872,  0.2591,  0.3496,  0.4283,  0.1915,  0.5106,\n",
      "         0.3150,  0.4725,  0.4543,  0.1366,  0.2693,  0.4801,  0.4162,  0.4636,\n",
      "         0.5499,  0.4447,  0.5408,  0.3388,  0.3328,  0.2696,  0.4282,  0.5429,\n",
      "         0.4895,  0.2485,  0.4448,  0.2772,  0.4292,  0.2427,  0.2279,  0.2768,\n",
      "         0.0479,  0.0340,  0.3004, -0.0984,  0.1594,  0.0339,  0.2678,  0.0518,\n",
      "         0.0608, -0.1932,  0.1089,  0.1717, -0.3606, -0.1579, -0.2084,  0.4391,\n",
      "        -0.0840, -0.1650,  0.2711,  0.3680, -0.0923, -0.2583, -0.1380,  0.3552,\n",
      "        -0.1174, -0.0664, -0.1112,  0.2865,  0.0713,  0.0077,  0.0091, -0.1743,\n",
      "         0.1253, -0.1685, -0.2600,  0.1852, -0.1119, -0.0938,  0.3080, -0.2055,\n",
      "         0.0772,  0.1263,  0.2921,  0.3443,  0.3593,  0.1499,  0.0350, -0.2127,\n",
      "        -0.1985, -0.1021, -0.0894,  0.0143,  0.2320,  0.3483,  0.1078, -0.2074,\n",
      "        -0.0675,  0.1478, -0.1689, -0.0438, -0.0908,  0.2780,  0.0023,  0.0401,\n",
      "         0.2222,  0.2684,  0.3130, -0.2467,  0.4296,  0.1371,  0.3216,  0.0393,\n",
      "        -0.0694, -0.0958,  0.2162, -0.2238, -0.1064, -0.1039, -0.0421, -0.3691,\n",
      "        -0.1365,  0.1686,  0.3232, -0.1426,  0.0678, -0.0622, -0.1786, -0.3385,\n",
      "        -0.1112, -0.1119,  0.0522, -0.1982, -0.1604,  0.1473,  0.2087, -0.0389,\n",
      "         0.1119,  0.1431, -0.2352,  0.2387,  0.1011, -0.1254,  0.2121,  0.1546,\n",
      "        -0.0593,  0.2295, -0.1571,  0.2446,  0.2220, -0.0945, -0.0536,  0.0950,\n",
      "         0.1423, -0.1782,  0.1236,  0.2561,  0.3398, -0.3727, -0.0089, -0.1356,\n",
      "        -0.0938,  0.1892,  0.0272, -0.1728,  0.0904,  0.3305, -0.0061, -0.1069,\n",
      "         0.4687,  0.6800,  0.1387,  0.5139,  0.1076,  0.5756,  0.0471,  0.5579,\n",
      "         0.3954,  0.5387,  0.4468,  0.3615,  0.6788,  0.4333,  0.5063,  0.0054,\n",
      "         0.5885,  0.6097,  0.0842,  0.2727,  0.4823,  0.4732,  0.6843,  0.1775,\n",
      "         0.6037,  0.4897,  0.6427,  0.2066,  0.4338,  0.2865,  0.6998,  0.7055,\n",
      "         0.2015,  0.7487,  0.6384,  0.2603,  0.7735,  0.5717,  0.3400,  0.6116,\n",
      "         0.3274,  0.2467,  0.0834,  0.2407,  0.0444,  0.3955,  0.6639,  0.7952,\n",
      "         0.6649,  0.6085,  0.6839,  0.7174,  0.2632,  0.0343,  0.6228,  0.4994,\n",
      "         0.5871,  0.1169,  0.5263,  0.3158,  0.6215, -0.0559,  0.6114,  0.4672],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc.weight Parameter containing:\n",
      "tensor([[-0.3409, -0.0038, -0.0592,  ...,  0.2352,  0.0852, -0.0542],\n",
      "        [-0.4274, -0.2114,  0.2045,  ..., -0.4423,  0.0181,  0.0664],\n",
      "        [-0.2580, -0.1757, -0.2342,  ..., -0.4947,  0.1708,  0.1492],\n",
      "        ...,\n",
      "        [-0.0021, -0.0806,  0.0307,  ..., -0.0612, -0.1257, -0.0631],\n",
      "        [ 0.2390, -0.2417, -0.2397,  ..., -0.1510,  0.1434,  0.3124],\n",
      "        [ 0.2402, -0.0386, -0.8729,  ..., -0.0228,  0.1909, -0.1981]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc.bias Parameter containing:\n",
      "tensor([-2.4955e-03,  9.5033e-02,  2.7374e-02,  6.9333e-02, -2.0681e-01,\n",
      "        -1.9942e-02, -1.6822e-01,  1.2130e-01,  7.7800e-02, -1.8492e-01,\n",
      "        -1.0028e-01,  3.0138e-02, -7.8384e-04, -1.6840e-01,  1.1945e-02,\n",
      "        -1.2422e-02, -1.0394e-01,  2.9764e-02,  6.8980e-03, -1.1669e-01,\n",
      "        -1.3701e-01, -1.1507e-01, -6.2383e-02, -2.4273e-01, -1.4080e-01,\n",
      "         2.5183e-01, -1.8517e-02,  7.6103e-02,  9.1190e-02,  1.3795e-01,\n",
      "         1.8331e-01,  2.6489e-01,  2.8383e-02,  1.3948e-01, -2.1002e-01,\n",
      "         9.6393e-02,  1.2178e-01,  1.9634e-01, -9.1127e-02,  1.2871e-01,\n",
      "        -2.5251e-02, -2.4975e-01,  3.5928e-02,  5.6112e-02, -5.3398e-02,\n",
      "         2.0266e-01,  2.9027e-02,  1.1064e-01,  3.0267e-02,  8.0304e-02,\n",
      "        -5.3328e-02, -2.1978e-01, -1.7872e-01,  1.0532e-01,  1.4870e-01,\n",
      "        -2.0583e-03,  1.8990e-01,  1.0497e-01,  2.3892e-01,  5.1464e-02,\n",
      "         6.2283e-02,  1.9018e-02,  5.4183e-02, -6.8693e-03,  3.8022e-02,\n",
      "         1.2101e-01,  8.1196e-02, -3.4678e-03,  8.9839e-02, -5.2063e-02,\n",
      "         1.3111e-01,  2.1456e-01,  2.7493e-02,  5.2901e-03,  6.9770e-02,\n",
      "         1.1595e-01,  1.6610e-01,  9.6886e-02,  2.0539e-01, -1.6225e-03,\n",
      "         8.7348e-02,  1.3410e-01,  6.9289e-02,  5.9926e-04,  3.5540e-02,\n",
      "        -1.1452e-01, -1.1053e-01, -2.9091e-02,  9.2430e-02,  1.9916e-02,\n",
      "         5.1885e-02, -3.0891e-02,  2.9333e-01,  4.3057e-02,  1.1245e-01,\n",
      "         7.3435e-03,  2.0411e-01,  8.8890e-02, -3.9854e-02,  1.8291e-02,\n",
      "         1.4009e-01, -5.9434e-02, -2.5541e-02,  4.5075e-02,  2.0953e-01,\n",
      "        -1.2079e-01, -1.8230e-01, -4.5195e-02,  1.5401e-01, -1.1874e-02,\n",
      "        -4.4966e-04,  5.1913e-02,  8.5713e-03,  1.2393e-02, -8.8101e-02,\n",
      "         7.5345e-02, -1.2827e-01,  1.5281e-01, -5.6047e-02,  2.6059e-03,\n",
      "         1.3381e-01, -8.1864e-02, -8.0614e-02, -1.0770e-01,  2.5352e-01,\n",
      "         6.6273e-02, -2.1712e-01, -6.9472e-03,  1.7542e-01,  1.7858e-01,\n",
      "         9.5384e-03, -1.2811e-01, -1.0821e-01,  7.3028e-02, -9.0888e-02,\n",
      "        -4.2347e-02,  2.0390e-01,  4.4884e-02, -1.0349e-01,  7.5631e-02,\n",
      "         6.6812e-02, -2.6410e-02, -1.5684e-01,  7.1880e-04, -7.7767e-02,\n",
      "         5.1424e-02, -1.1131e-01,  7.4391e-02,  1.6110e-01, -2.0981e-02,\n",
      "        -3.1687e-02, -1.8797e-01, -2.0018e-01, -1.8394e-01,  2.3867e-05,\n",
      "         4.1529e-02, -5.9845e-02,  1.0053e-01, -1.9250e-01, -1.1285e-01,\n",
      "        -7.4909e-02,  3.6765e-02, -7.5817e-02, -2.7927e-02, -8.3274e-02,\n",
      "        -1.2786e-01, -1.2711e-01,  6.7402e-03,  3.1988e-03, -1.7276e-01,\n",
      "        -1.0900e-01, -1.1322e-02,  1.5034e-01, -9.3802e-02, -6.4943e-02,\n",
      "        -3.3355e-03, -7.9784e-02, -1.3968e-01,  1.7211e-02,  6.8969e-02,\n",
      "         6.6460e-02, -1.3546e-01, -5.6365e-02, -1.7529e-01, -1.6420e-01,\n",
      "         4.8902e-02,  4.2027e-02,  9.5117e-02, -6.1900e-02,  1.1402e-01,\n",
      "        -1.5510e-01, -3.3247e-03,  5.7054e-02,  1.5053e-01, -7.1104e-02,\n",
      "        -2.2286e-01, -3.6419e-02, -7.2270e-02, -2.0195e-01,  1.2368e-02,\n",
      "        -6.1687e-02, -9.4573e-03, -4.4656e-03, -1.3234e-01, -5.3868e-02,\n",
      "        -1.9360e-01,  4.0520e-02, -9.9554e-02, -7.6786e-02,  3.2833e-02,\n",
      "         3.3978e-03,  8.8934e-02,  1.1633e-01, -1.5751e-01,  8.2773e-02,\n",
      "        -1.7566e-03,  2.2547e-02,  7.3851e-03, -4.2136e-03, -2.6790e-01,\n",
      "        -5.1355e-04, -2.9170e-02,  2.4412e-02, -7.3106e-02, -5.4447e-02,\n",
      "        -8.8234e-02, -4.6222e-02, -1.5781e-02, -1.3568e-02, -2.5119e-01,\n",
      "        -1.2051e-01, -8.9391e-02,  1.2574e-02, -5.2829e-02, -2.5953e-02,\n",
      "        -1.3947e-01, -2.3489e-01, -4.7935e-02, -6.4324e-02, -1.2445e-01,\n",
      "        -2.7073e-02,  5.9472e-03, -5.0026e-02, -1.6814e-02, -8.3109e-02,\n",
      "        -4.3938e-02,  1.6542e-02, -1.2117e-01,  5.7050e-02,  5.6689e-02,\n",
      "         1.5569e-01,  2.1700e-02,  7.4942e-02, -8.9662e-02, -1.3717e-01,\n",
      "        -9.7561e-02,  4.4743e-02, -1.5978e-01, -9.9398e-03, -1.7278e-01,\n",
      "         1.1248e-02, -9.1754e-02, -4.1269e-02, -1.7859e-01,  3.5096e-03,\n",
      "         1.2789e-03, -1.9969e-01, -3.0675e-02,  4.8002e-02, -3.2221e-02,\n",
      "        -7.9431e-02, -7.3198e-02, -9.7312e-02,  1.0536e-02, -6.5567e-02,\n",
      "        -3.3420e-02, -1.3808e-01,  4.9554e-02, -5.2369e-02, -3.3894e-04,\n",
      "        -1.0350e-01, -3.2216e-02, -4.6480e-02, -1.5378e-01, -2.6191e-01,\n",
      "         4.9975e-03,  2.5357e-02, -4.3472e-02, -3.7670e-02,  8.4378e-02,\n",
      "         6.1441e-02,  1.2118e-01,  5.4270e-02, -5.3488e-02, -1.9215e-01,\n",
      "        -8.4701e-02, -1.2274e-01, -1.6000e-02, -1.0990e-01,  5.6575e-03,\n",
      "        -1.2473e-02, -1.3351e-01, -7.4632e-02, -1.4924e-01, -1.1345e-01,\n",
      "        -1.6778e-01, -1.4079e-01, -2.2746e-01, -1.2537e-01, -5.3346e-02,\n",
      "         1.4860e-03, -7.5198e-02, -6.8133e-02, -1.2624e-01,  8.8154e-02,\n",
      "        -1.7134e-01,  3.6822e-02, -2.3525e-01,  2.5628e-02, -2.1162e-01,\n",
      "        -1.4706e-01, -4.0332e-04,  4.9281e-02, -3.4148e-01, -3.1726e-02,\n",
      "         8.9492e-02,  6.4112e-04, -1.4835e-01, -1.3112e-01, -1.3169e-01,\n",
      "        -8.9510e-02,  6.8039e-02, -1.9052e-01, -1.1963e-02, -1.7951e-01,\n",
      "         7.0678e-02, -3.8217e-02, -1.5814e-01,  1.8694e-02, -2.5429e-01,\n",
      "        -2.5376e-01, -7.7069e-02, -6.8384e-02, -9.5146e-02, -8.0866e-02,\n",
      "        -8.2601e-02, -2.5534e-01, -6.2697e-02, -1.3400e-01, -2.1340e-01,\n",
      "         1.7579e-02, -2.9307e-01, -1.0576e-01, -8.7128e-02, -7.7835e-02,\n",
      "        -1.7855e-01, -3.9648e-02, -1.7829e-01, -2.3355e-02, -2.5755e-01,\n",
      "        -5.8789e-02, -1.0632e-01, -2.1917e-02, -2.0641e-01,  8.9925e-02,\n",
      "        -1.3262e-01, -1.0559e-01,  3.6313e-03, -1.4526e-01, -1.1547e-02,\n",
      "        -7.9769e-02, -1.2643e-01, -5.4181e-02, -5.5778e-02, -8.3455e-02,\n",
      "        -1.6359e-01, -3.1428e-01, -1.2808e-01, -3.0718e-02,  4.2125e-02,\n",
      "        -3.7430e-02,  1.5905e-01,  9.3671e-02, -5.7554e-02, -1.3361e-01,\n",
      "        -1.6027e-01, -7.1707e-02, -8.9638e-02, -2.5058e-01, -7.6269e-03,\n",
      "        -6.2544e-02, -7.9640e-02, -2.0489e-01, -1.3257e-01, -1.4670e-01,\n",
      "        -1.5351e-01, -1.5681e-01, -2.1577e-01,  9.9802e-02, -6.7230e-02,\n",
      "        -8.5613e-02, -5.8299e-02, -7.0785e-03, -2.8866e-03, -1.6394e-01,\n",
      "        -1.1881e-01, -1.3872e-01,  6.5054e-02,  7.9950e-02, -4.4925e-02,\n",
      "        -1.0826e-01, -1.1446e-01, -1.2438e-01, -8.8174e-02, -1.4826e-01,\n",
      "        -2.0981e-01, -4.7578e-02,  2.8893e-02, -1.8978e-02,  6.0667e-02,\n",
      "        -1.9644e-01, -7.8700e-02, -1.6383e-02, -1.6894e-01, -5.2443e-02,\n",
      "         5.9872e-03, -1.1586e-01, -1.8752e-01, -2.8632e-01, -1.4542e-01,\n",
      "        -5.8363e-03, -1.0812e-01,  9.7533e-02, -1.7418e-01,  1.1240e-02,\n",
      "        -1.1633e-01, -8.5669e-02, -8.1105e-02, -1.5369e-01, -6.2729e-02,\n",
      "         8.1007e-02, -1.6660e-02, -1.9627e-01, -8.3040e-02,  6.8984e-03,\n",
      "        -1.2417e-01, -8.7582e-02, -3.4696e-02,  4.0263e-02, -2.4088e-01,\n",
      "         2.0830e-02, -1.4456e-01, -9.0805e-02, -4.5083e-03, -3.8755e-01,\n",
      "        -1.2003e-01, -1.5955e-01, -2.3558e-01, -3.0268e-01, -8.5077e-02,\n",
      "         8.4440e-02, -1.5927e-01, -1.4362e-01, -2.0858e-01, -4.2387e-02,\n",
      "        -2.3530e-01, -8.0401e-02, -1.7736e-01, -9.4342e-02, -1.0171e-01,\n",
      "        -5.3377e-03, -2.1040e-01, -1.1206e-01, -1.7141e-01, -1.7010e-01,\n",
      "        -1.7659e-01,  7.1540e-02, -4.0044e-02, -1.6377e-01, -4.9828e-02,\n",
      "         4.8558e-02, -2.6821e-01, -3.3051e-02,  1.4054e-01, -1.1480e-01,\n",
      "        -4.4082e-02, -1.6673e-02, -3.1377e-03, -1.1988e-01, -8.7943e-02,\n",
      "        -1.8613e-01, -5.0183e-03, -2.1311e-02, -1.6363e-01, -8.7199e-03,\n",
      "        -3.1601e-01, -1.9182e-02, -1.5357e-01, -2.2188e-01,  8.3488e-02],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
