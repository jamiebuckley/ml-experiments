{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T01:35:32.043044Z",
     "start_time": "2025-08-29T01:35:27.697944Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def demonstrate_packing():\n",
    "    print(\"=== PACKED SEQUENCE INTERNALS ===\\n\")\n",
    "\n",
    "    # Create sample sequences with different lengths\n",
    "    sequences = [\n",
    "        torch.tensor([[1.0], [2.0], [3.0]]),           # length 3\n",
    "        torch.tensor([[4.0], [5.0]]),                  # length 2\n",
    "        torch.tensor([[6.0], [7.0], [8.0], [9.0]])     # length 4\n",
    "    ]\n",
    "\n",
    "    # Pad them normally\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    lengths = torch.tensor([3, 2, 4])\n",
    "\n",
    "    print(\"1. ORIGINAL PADDED DATA:\")\n",
    "    print(\"Shape:\", padded.shape)  # [3, 4, 1] - batch_size, max_len, features\n",
    "    print(\"Data:\")\n",
    "    for i, seq in enumerate(padded):\n",
    "        print(f\"  Batch {i}: {seq.squeeze().tolist()} (length: {lengths[i]})\")\n",
    "    print()\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed = pack_padded_sequence(padded, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    print(\"2. AFTER PACKING:\")\n",
    "    print(\"Packed data shape:\", packed.data.shape)  # [9, 1] - total_elements, features\n",
    "    print(\"Packed batch_sizes:\", packed.batch_sizes.tolist())\n",
    "    print(\"Packed sorted_indices:\", packed.sorted_indices.tolist())\n",
    "    print(\"Packed unsorted_indices:\", packed.unsorted_indices.tolist())\n",
    "    print()\n",
    "\n",
    "    print(\"3. HOW THE DATA IS REORGANIZED:\")\n",
    "    print(\"Original padded layout:\")\n",
    "    print(\"  Timestep 0: [1.0, 4.0, 6.0]  <- all 3 sequences\")\n",
    "    print(\"  Timestep 1: [2.0, 5.0, 7.0]  <- all 3 sequences\")\n",
    "    print(\"  Timestep 2: [3.0, 0.0, 8.0]  <- 2 sequences + padding\")\n",
    "    print(\"  Timestep 3: [0.0, 0.0, 9.0]  <- 1 sequence + padding\")\n",
    "    print()\n",
    "\n",
    "    print(\"Packed layout (no padding!):\")\n",
    "    data_flat = packed.data.squeeze().tolist()\n",
    "    print(f\"  Packed data: {data_flat}\")\n",
    "\n",
    "    print(\"\\nHow to read packed data:\")\n",
    "    idx = 0\n",
    "    for t, batch_size in enumerate(packed.batch_sizes):\n",
    "        timestep_data = data_flat[idx:idx+batch_size]\n",
    "        print(f\"  Timestep {t}: {timestep_data} <- {batch_size} active sequences\")\n",
    "        idx += batch_size\n",
    "    print()\n",
    "\n",
    "    return packed, padded, lengths\n",
    "\n",
    "def demonstrate_lstm_processing():\n",
    "    print(\"4. LSTM PROCESSING COMPARISON:\\n\")\n",
    "\n",
    "    packed, padded, lengths = demonstrate_packing()\n",
    "\n",
    "    # Create LSTM\n",
    "    lstm = nn.LSTM(input_size=1, hidden_size=2, batch_first=True)\n",
    "\n",
    "    # Method 1: Process padded data (includes padding computation)\n",
    "    print(\"Method 1 - Padded processing:\")\n",
    "    with torch.no_grad():\n",
    "        padded_output, _ = lstm(padded)\n",
    "    print(f\"  Padded output shape: {padded_output.shape}\")\n",
    "    print(f\"  Total computations: {padded_output.shape[0] * padded_output.shape[1]} timesteps\")\n",
    "    print(\"  (Includes wasted computation on padding!)\")\n",
    "    print()\n",
    "\n",
    "    # Method 2: Process packed data (no padding computation)\n",
    "    print(\"Method 2 - Packed processing:\")\n",
    "    with torch.no_grad():\n",
    "        packed_output, _ = lstm(packed)\n",
    "    print(f\"  Packed output shape: {packed_output.data.shape}\")\n",
    "    print(f\"  Total computations: {packed_output.data.shape[0]} timesteps\")\n",
    "    print(\"  (Only real data, no padding!)\")\n",
    "    print()\n",
    "\n",
    "    # Unpack to compare\n",
    "    unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "    print(\"After unpacking:\")\n",
    "    print(f\"  Unpacked shape: {unpacked_output.shape}\")\n",
    "    print()\n",
    "\n",
    "    print(\"5. COMPUTATION SAVINGS:\")\n",
    "    padded_ops = padded.shape[0] * padded.shape[1]  # batch * max_len\n",
    "    packed_ops = packed.data.shape[0]  # total real elements\n",
    "    savings = (padded_ops - packed_ops) / padded_ops * 100\n",
    "    print(f\"  Padded approach: {padded_ops} operations\")\n",
    "    print(f\"  Packed approach: {packed_ops} operations\")\n",
    "    print(f\"  Savings: {savings:.1f}%\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_lstm_processing()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. LSTM PROCESSING COMPARISON:\n",
      "\n",
      "=== PACKED SEQUENCE INTERNALS ===\n",
      "\n",
      "1. ORIGINAL PADDED DATA:\n",
      "Shape: torch.Size([3, 4, 1])\n",
      "Data:\n",
      "  Batch 0: [1.0, 2.0, 3.0, 0.0] (length: 3)\n",
      "  Batch 1: [4.0, 5.0, 0.0, 0.0] (length: 2)\n",
      "  Batch 2: [6.0, 7.0, 8.0, 9.0] (length: 4)\n",
      "\n",
      "2. AFTER PACKING:\n",
      "Packed data shape: torch.Size([9, 1])\n",
      "Packed batch_sizes: [3, 3, 2, 1]\n",
      "Packed sorted_indices: [2, 0, 1]\n",
      "Packed unsorted_indices: [1, 2, 0]\n",
      "\n",
      "3. HOW THE DATA IS REORGANIZED:\n",
      "Original padded layout:\n",
      "  Timestep 0: [1.0, 4.0, 6.0]  <- all 3 sequences\n",
      "  Timestep 1: [2.0, 5.0, 7.0]  <- all 3 sequences\n",
      "  Timestep 2: [3.0, 0.0, 8.0]  <- 2 sequences + padding\n",
      "  Timestep 3: [0.0, 0.0, 9.0]  <- 1 sequence + padding\n",
      "\n",
      "Packed layout (no padding!):\n",
      "  Packed data: [6.0, 1.0, 4.0, 7.0, 2.0, 5.0, 8.0, 3.0, 9.0]\n",
      "\n",
      "How to read packed data:\n",
      "  Timestep 0: [6.0, 1.0, 4.0] <- 3 active sequences\n",
      "  Timestep 1: [7.0, 2.0, 5.0] <- 3 active sequences\n",
      "  Timestep 2: [8.0, 3.0] <- 2 active sequences\n",
      "  Timestep 3: [9.0] <- 1 active sequences\n",
      "\n",
      "Method 1 - Padded processing:\n",
      "  Padded output shape: torch.Size([3, 4, 2])\n",
      "  Total computations: 12 timesteps\n",
      "  (Includes wasted computation on padding!)\n",
      "\n",
      "Method 2 - Packed processing:\n",
      "  Packed output shape: torch.Size([9, 2])\n",
      "  Total computations: 9 timesteps\n",
      "  (Only real data, no padding!)\n",
      "\n",
      "After unpacking:\n",
      "  Unpacked shape: torch.Size([3, 4, 2])\n",
      "\n",
      "5. COMPUTATION SAVINGS:\n",
      "  Padded approach: 12 operations\n",
      "  Packed approach: 9 operations\n",
      "  Savings: 25.0%\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
